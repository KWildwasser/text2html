<!DOCTYPE html>
<html lang="de">
<meta charset=UTF-8>
<body>
<h2>4.2 Naives Bayes Verfahren</h2>
<p>Voraussetzungen Bayes-Regel</p>
<p>Vorbemerkung: Dieses Verfahren nimmt die Unabhängigkeit aller Argumentattribute voneienander an.</p>
<p>Verfahren:</p>
<ul>
<li>Erst einmal Statistik aufstellen, wie häufig jedes Argumentattribut einen bestimmten Wert des Klassifikationsattributs nach sich zieht.</li>
<li>Hieraus wird für jedes Argument-Argument brechnet, welchen Antei jeder seiner Werte an einem bestimmten Wert des Klassifikationsattributs aht.</li>
<li>Danach erfolgt die Anwendung der Bayes-Regel, um für "neue Daten" (z.B. einen "neuen Tag") zu berechnen:</li>
</ul>
<p>P(play=yes | 'neue Wetterbedingungen')=</p>
<p>P(play=no | 'neue Wetterbedingungen')=</p>
<p>Schlussendlich Entscheidung: für die grössere dieser beiden Wahrscheinlichkeiten.</p>

<h2>4.3 Berechnung von Entscheidungsbäumen (Id3-Alorithmus)</h2>
<p>Ein Entscheidungsbaum berücksichtigt mehrere Argument-Attribute in unterschiedlicher Wichtigkeit zur Entscheidungsfindung.</p>
<p>Voraussetzung für den Aufbau eines Entscheidungsbaums sind zwei Funktionen:</p>
<ul>
<li>info(): Maß (Kennzahl) für während eines Lernprozesses noch zu gewinnende Information.</li>
<li>gain(): Maß (Kennzahl) für Informationsgewinn (i.A. Differenz zweier Werte der info()-Funktion).</li>
</ul>
<p>Verfahren:</p>
<ul>
<li>Anfangsberechnung für die noch zu gewinnende Information: info(9,5) für 9 yes-Fälle, 5 no-Fälle in den Trainingsdaten</li>
<li>Berechnung des Entscheidungsbaums: zunächstr Berechnung des "wichtigsten" Arributs, das in die Wurzel des sich entwickelnden Entscheinungsbaums eingefügt wird (outlook) über die info()- und gain()-Funktionen.</li>
<li>Analog weiter für Teilbäume, jeweils für die Werte des Argumentattributd (ootlook)</li>
<li>Ende der Entwicklung des Entscheidungsbaums: wenn alle Blattknoten nur noch einen Wer des Klassifikationsattribus enthalten.</li>
</ul>
<p>Dieser Id3-Algorithmus ist otimiert worden: C4.5, C5.0, J48</p>
</body>
</html>
